{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.9.0', '2.1.6-tf')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import Lambda,Permute,Multiply\n",
    "from tensorflow.keras.layers import Input,Dense,Flatten,Softmax,Embedding,Reshape,SimpleRNN,GRU,LSTM,Bidirectional,Dropout,TimeDistributed\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "tf.__version__, K.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
    "sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
    "\n",
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "number_dict = {i: w for i, w in enumerate(word_list)}\n",
    "n_class = len(word_dict)  # vocab list\n",
    "\n",
    "# Parameter\n",
    "n_step = 5  # maxium number of words in one sentence(=number of time steps)\n",
    "n_hidden = 128\n",
    "\n",
    "def make_batch(sentences):\n",
    "    input_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[0].split()]]]\n",
    "    output_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[1].split()]]]\n",
    "    target_batch = [[word_dict[n] for n in sentences[2].split()]]\n",
    "    return np.array(input_batch), np.array(output_batch), np.array(target_batch).reshape(1,-1,1)\n",
    "\n",
    "input_batch, output_batch, target_batch = make_batch(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras 自定义层示意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "class MyLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # 为该层创建一个可训练的权重\n",
    "#         self.kernel = self.add_weight(name='kernel', \n",
    "#                                       shape=(input_shape[1], self.output_dim),\n",
    "#                                       initializer='uniform',\n",
    "#                                       trainable=True)\n",
    "        self.kernel = self.add_weight(name='atteention_weight', \n",
    "                                      shape=(input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # 一定要在最后调用它\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.dot(x, self.kernel)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs, single_attention_vector=False):\n",
    "    # 如果上一层是LSTM，需要return_sequences=True\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim) = (?, 5, 128)\n",
    "    time_steps = np.shape(inputs)[1]\n",
    "    input_dim = np.shape(inputs)[2]\n",
    "    a = Permute((2, 1))(inputs) # shape = (?, 128, 5)\n",
    "    a = Dense(time_steps, activation='softmax')(a) # shape = (?, 128, 5)\n",
    "    if single_attention_vector:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1))(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "\n",
    "    a_probs = Permute((2, 1))(a) # shape = (?, 5, 128)\n",
    "    print('a_probs维度：',np.shape(a_probs))\n",
    "    # 乘上了attention权重，但是并没有求和，好像影响不大\n",
    "    # 如果分类任务，进行Flatten展开就可以了\n",
    "    # element-wise\n",
    "    output_attention_mul = Multiply()([inputs, a_probs]) # shape = (?, 5, 128)\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        # W.shape = (time_steps, time_steps)\n",
    "        self.W = self.add_weight(name='att_weight', \n",
    "                                 shape=(input_shape[1], input_shape[1]),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', \n",
    "                                 shape=(input_shape[1],),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs.shape = (batch_size, time_steps, seq_len)\n",
    "        x = K.permute_dimensions(inputs, (0, 2, 1))\n",
    "        # x.shape = (batch_size, seq_len, time_steps)\n",
    "        a = K.softmax(K.tanh(K.dot(x, self.W) + self.b))\n",
    "        outputs = K.permute_dimensions(a * x, (0, 2, 1))\n",
    "        outputs = K.sum(outputs, axis=1)\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention GRU network       \n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        \n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init： (?, 5, 128)\n",
      "第一次维度： (?, 128, 5)\n",
      "Dense后维度： (?, 128, 5)\n",
      "a_probs维度： (?, 5, 128)\n",
      "权重维度： (?, 5, 128)\n",
      "输出维度： (?, 5, 128)\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.3958 - acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.3801 - acc: 0.6000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.3662 - acc: 0.6000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.3498 - acc: 0.6000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.3272 - acc: 0.6000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.2922 - acc: 0.8000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.2330 - acc: 0.8000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.1318 - acc: 0.6000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.9872 - acc: 0.6000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.8430 - acc: 0.6000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.7230 - acc: 0.4000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.6195 - acc: 0.4000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.5262 - acc: 0.4000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.4423 - acc: 0.8000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3641 - acc: 0.8000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.3141 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2839 - acc: 0.6000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2285 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2017 - acc: 0.8000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1602 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.1371 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1075 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0910 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.0662 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0520 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.0271 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.0127 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.9955 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.9845 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9639 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9526 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.9360 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.9290 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.9109 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.9012 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.8850 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.8769 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.8604 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.8531 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.8393 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8339 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.8165 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.8093 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7943 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7872 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7718 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7684 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7553 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7494 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7344 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7266 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7138 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7076 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6962 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6906 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6778 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6738 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6598 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6536 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6397 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6329 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6211 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6175 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6070 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6049 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5936 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5906 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5741 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5659 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5526 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5448 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5354 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5312 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5238 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5246 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5145 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5136 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4966 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.4877 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4742 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4667 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4574 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4520 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4456 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.4440 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4375 - acc: 1.0000\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4366 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4255 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.4224 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4077 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3999 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3902 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3843 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3775 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3750 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3675 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3647 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3566 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3542 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3445 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcf1446da20>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input = Input(shape=(n_step, n_class)) # [batch_size, n_step, n_lcass]\n",
    "decoder_input = Input(shape=(n_step, n_class))\n",
    "\n",
    "encoder = LSTM(n_hidden, return_sequences=True, return_state=True)\n",
    "# dropout\n",
    "encoder_output, state_h, state_c = encoder(encoder_input)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "decoder = LSTM(n_hidden, return_sequences=True)\n",
    "decoder_output = decoder(decoder_input, initial_state = encoder_state)\n",
    "\n",
    "# attention = attention_3d_block(())\n",
    "attention_output = attention_3d_block(decoder_output)\n",
    "\n",
    "dense = Dense(n_class, activation='softmax')\n",
    "outputs = dense(attention_output)\n",
    "\n",
    "model = Model([encoder_input, decoder_input], outputs)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "model.fit([input_batch, output_batch], target_batch, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ich', 'mochte', 'ein', 'bier', 'P'] -> ['i', 'want', 'a', 'beer', 'E']\n"
     ]
    }
   ],
   "source": [
    "predict_batch = [np.eye(n_class)[[word_dict[n] for n in 'P P P P P'.split()]]]\n",
    "result = model.predict([input_batch, np.array(predict_batch)])\n",
    "result = np.argmax(result, axis=2)\n",
    "print(sentences[0].split(), '->', [number_dict[n] for n in result[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
